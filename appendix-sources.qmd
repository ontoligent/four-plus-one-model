---
title: "Primary Sources"
format: html
editor: visual
link-citations: true
bibliography: HistoryDataScience.bib
execute:
  echo: false
---

# About the Sources {.unnumbered}

The primary sources on which the conclusions of this essay are based comprise a variety of documents, from technical journals to blog posts to internal reports. They also come from a range of viewpoints, from data analysis and statistics to data mining and data science *per se*. For the purposes of the essay, we select a more or less representative subset across these axes of variation. With respect to representativeness, in some cases a document was chosen for its influence, in others, such as the post by Dataman, because it is considered more or less typical of common genre. 

The twelve documents chosen are listed in chronological order, beginning with Tukey's seminal essay on data analysis and ending with contempory explainers. Included also are the definitions of the CRISP-DM and KDD processes which are the most developed pipeline models.

Each source entry below contains short description and then list the phases cited by the authors as fundamental to data processing, broadly conceived. We then map these phases onto the standard sequence described in the main part of this essay and listed here for convenience. 

1.  Understand
2.  Plan

3.  Collect
4.  Store
5.  Clean
6.  Explore
7.  Prepare

8.  Model
9.  Interpret

10. Communicate
11. Deploy

12. Reflect

These mappings are indicated by an arrow pointing to the subset of terms from the standard sequence, e.g. ... $\rightarrow [Explore]$ These mappings are also aggregated into a composite pipeline and displayed the table below;each model row is referenced by its key as defined in the entries. 

It should be noted that in most cases these phases are explicitly described as a process and often as a pipeline. When they are not, the implication is strong. In some cases, the process is likened to a cycle, emphasizing the connection between the endpoints of the pipeline, which is also emphasized by the 4+1 model.

```{python}
phases = "Understand Plan Collect Store Clean Explore Prepare Model Interpret Communicate Deploy Reflect".split()
sources = "Tukey KDD SEMMA Hayashi CRISPDM OSEMI Ojeda+ Caffo+ Donoho Géron Das Dataman Porter".split()
pipes = {source:'' for source in sources}
```

## List of Sources

### Tukey on Data Analysis

Key: `Tukey`\
Year: 1962 \
Source: @tukeyFutureDataAnalysis1962 [URL](https://www.jstor.org/stable/2237638#metadata_info_tab_contents)\
Field: Statistics, Data Analysis

In this classic essay, Tukey introduces the concept of data analysis, which he distinguishes from mathematical statistics and likens to an empirical science. He defines data analysis as an empirical process with phases including "... procedures for **analyzing** data, techniques for **interpreting** the results of such procedures, ways of **planning** the **gathering** of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data" (p. 2). Unpacking this statement yields a four phase model.

1.  **Planning**: This phase includes "ways of planning the the gather of data to make its analysis easier." $\rightarrow [Plan]$ 
2.  **Gathering**: The gathering of data, either through creation or by acquisition of "data already obtained" (p. 40). Includes also the shaping of data "to make its analysis easier," which corresponds to our concept of Preparation.
$\rightarrow [Collect]$ 
3.  **Analyzing**: This is where data are analyzed with "all the machinery and results of (mathematical) statistics."
$\rightarrow [Explore, Model]$ 
4.  **Interpreting**: "techniques for interpreting the results of" analysis.
$\rightarrow [Interpret]$ 

Note that Tukey's model is unique in that his concept of exploration carries much more weight than how the term tends to be used to day. 

```{python}
pipes['Tukey'] = """
Understand | 
Plan | planning
Collect | gathering
Store | 
Clean | 
Explore | analyzing
Prepare | gathering
Model | analyzing
Interpret | interpret
Communicate | 
Deploy | 
Reflect | 
"""
```

### Fayyad on KDD

Key: `KDD`\
Year: 1996
Source: @fayyadKnowledgeDiscoveryData1996 [URL→](https://www.aaai.org/Papers/KDD/1996/KDD96-014.pdf?utm_campaign=ml4devs-newsletter&utm_medium=email&utm_source=Revue%20newsletter)\
Field: Data Mining\

KDD, or Knowledge Discovery in Databases, emerged in the late 1980s as both datasets and the computational resources to work with them became abundant. These resources included commercial databases and personal computers. In many ways the most adjacent field to contemoporary data science, this approach is unabashedly dedicated to finding patterns in data prior to developing a probabilistic model to justify their use. Fayyad's essay identifies five steps [@fayyadKnowledgeDiscoveryData1996: 84]. He emphasizes the highly iterative and cyclical nature of the process, arguing that it "may contain loops between any two steps." Another significant aspect of this conception of the pipeline is the role of exploration in the analytical phase: "Data Mining is a step in the KDD process consisting of applying data analysis and discovery algorithms that, under acceptable computational efficiency limitations, produce a particular enumeration of patterns over the data ...." (p. 83)

1.  **Selection**: Creating a target data set, or focusing on a subset of variables or data samples, on which discovery is to be performed. 
$\rightarrow [Collect]$
2.  **Pre-processing**: Cleaning and pre processing the data in order to obtain consistent data.
$\rightarrow [Clean]$
3.  **Transformation**: Transformation of the data using dimensionality reduction and other methods.
$\rightarrow [Prepare]$
4.  **Data Mining**: Searching for patterns of interest in a particular representational form, depending on the DM objective (usually, prediction).
$\rightarrow [Model]$
5.  **Interpretation/Evaluation**: Interpretation and evaluation of the mined patterns.
$\rightarrow [Interpret]$

```{python}
pipes['KDD'] = """
Understand | 
Plan | 
Collect | selection 
Store | 
Clean | pre-processing
Explore | 
Prepare | transformation
Model | data mining
Interpret | interpretation and evaluation
Communicate | 
Deploy | 
Reflect | 
"""
```

### Azevedo on SEMMA

Key: `SEMMA`\
Year: 1996
Source: @azevedoKDDSEMMACRISPDM2008\
Field: Statistics

The SEMMA model was developed the by SAS institute in 1996 as part of the documentation for their product, SAS Enterprise Miner. Even so, the model is referenced outside of this context, often as a comparison to KDD and CRISP-DM. Its bias towards statististics is evident in the first step.

1.  **Sample**: Sampling the data by extracting a portion of a large data set big enough to contain the significant information, yet small enough to manipulate quickly.
$\rightarrow [Collect]$
2.  **Explore**: Exploration of the data by searching for unanticipated trends and anomalies in order to gain understanding and ideas
$\rightarrow [Explore]$
3.  **Modify**: Modification of the data by creating, selecting, and transforming the variables to focus the model selection process
$\rightarrow [Prepare]$
4.  **Model**: Modeling the data by allowing the software to search automatically for a combination of data that reliably predicts a desired outcome.
$\rightarrow [Model]$
5.  **Assess**: Assessing the data by evaluating the usefulness and reliability of the findings from the DM process and estimate how well it performs.
$\rightarrow [Interpret]$

```{python}
pipes['SEMMA'] = """
Understand | 
Plan | 
Collect | sample 
Store | 
Clean | 
Explore | explore
Prepare | modify
Model | model
Interpret | assess
Communicate | 
Deploy | 
Reflect | 
"""
```

### Hayashi on Data Science

Key: `Hayashi` \
Year: 1998 \
Source: @hayashiDataScienceClassification1998 [URL→](https://link.springer.com/chapter/10.1007/978-4-431-65950-1_3) \
Field: Statistics

The Japanese statistician Chikio Hayashi adopted the term "data science" in the early 1990s to define a field that did not succumb to what he saw to be the errors of both statistics and data analysis. He argued that mathematical statistics had become too attached to problems of inference and removed from reality, while data analysis had lost interest in understanding the meaning of the data it deals with. His definition of data science is decidely processual: "Data Science consists of three phases: design for data, collection of data and analysis on data. It is important that the three phases are treated with the concept of unification based on the fundamental philosophy of science .... In these phases the methods which are fitted for the object and are valid, must be studied with a good perspective." (p. 41) Similar to KDD and CRISM-PM, Hayashi envisioned this process as a spiral, oscillating between poles if what he called "diversification" and "simplification." Note also that each of these terms, as described, comprises more than on of the standard sequence phases.

1.  **Design**: Surveys and experiments are developed to capture data from "multifarious phenomena."
$\rightarrow [Understand, Plan]$
2.  **Collection**: Phenomena are expressed as multidimensional or time-series data; properties of the data are made clear. At this stage, data are too complicated to draw clear conclusions. (Representation)
$\rightarrow [Collect, Explore, Prepare]$
3.  **Analysis**: By methods of classification, multidimensional data analysis, and statistics, data structure is revealed. Simplification and conceptualization. Also yields understanding of deviations of the model, which begins the cycle anew. (Revelation)
$\rightarrow [Model, Interpet]$

```{python}

pipes['Hayashi'] = """
Understand | design
Plan | design
Collect | collection
Store | collection
Clean | 
Explore | collection
Prepare | collection
Model | analysis
Interpret | analysis 
Communicate | 
Deploy | 
Reflect | 
"""
```

### Wirth and Hipp on CRISP-DM

Key: `CRISPDM` \
Year: 1999 \
Source: @wirthCRISPDMStandardProcess1999 [URL→](http://www.cs.unibo.it/~danilo.montesi/CBD/Beatriz/10.1.1.198.5133.pdf) \
Field: Data Mining

By the late 1990s, the practice of data mining had become widespread in industry and globally. In 1999 the Cross Industry Standard Process for Data Mining (CRISP-DM) was developed in Europe as a comprehensive and general model to support the use of data mining in a broad range of sectors in a principled manner. Designed to work within a project management framework, this model is by far the most developed, and it continues to influence the field of data science to this day. Like KDD before it, the model emphasizes the cyclic and recursive nature of the process, and this perspective is reflected in the circular diagram that often accompanies its presentation. The steps below are based on the summary presented in Wirth and Hipp's essay.

![Process diagram showing the relationship between the different phases of CRISP-DM (Wikipedia)](images/1024px-CRISP-DM_Process_Diagram.png)

1.  **Business Understanding**: Understanding project objectives and requirements from a business perspective. Includes the development of a plan.
$\rightarrow [Understand, Plan]$
2.  **Data Understanding**: The initial data collection and activities to get familiar with the data, e.g. to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information. This is really two phases --- **Collection** and **Exploration** --- which are combined because of their close, iterative relationship.
$\rightarrow [Collect, Explore]$
3.  **Data Preparation**: Construction of the final dataset for analytical use. Tasks include table, record, an attribute selection, data cleaning, construction of new attributes, and transformation of data for modeling tools.
$\rightarrow [Clean, Prepare]$
4.  **Modeling**: Modeling techniques are selected and applied, parameters calibrated. Modeling techniques include a broad range of unsupervised and supervised methods. As with KDD, there is an emphasis on pattern discovery, which has the effect of promoted methods that other models place squarely in the Explore phase of the standard sequence.
$\rightarrow [Model]$
5.  **Evaluation**: Evaluation of model performance by both intrinsic and extrinsic measures. Regarding the latter, a key objective is to determine if an important business issue has not been sufficiently considered.
$\rightarrow [Interpret]$
6.  **Deployment**: The knowledge gained by the model is presented in a way that the customer can use it. This may be something as simple as a report or as complex as a repeatable data mining process. In many cases the user, not the data analyst, will carry out the deployment.
$\rightarrow [Deploy]$

```{python}
pipes['CRISPDM'] = """
Understand | Business understanding
Plan | Business Understanding
Collect | Data Understanding
Store | Data Preparation
Clean | Data Preparation
Explore | Data Understanding*
Prepare | Data Preparation
Model | Modeling
Interpret | Evaluation
Communicate | Deployment
Deploy | Deployment
Reflect | 
"""
```

### Mason and Wiggins on OSEMI

Key: `OSEMI` \
Year: 2010 \
Source: @masonTaxonomyDataScience2010 [URL→](https://sites.google.com/a/isim.net.in/datascience_isim/taxonomy) \
Field: Data Science

After the phrase "data science" went viral (circa 2009), there were many efforts to make sense of the idea. It is around this time (2010) that Conway posted his Venn diagram of data science [@conwayDataScienceVenn2010]. Another influence model, based explicitly on the pipeline, came from Mason and Wiggins in a blog post hosted at O'Reilly's Tech Radar site. In contrast to previous models rooted in statistics, this model assumes that data are abundant and available, such as data scrapable from the Web.

1.  **Obtain**: Gather data from relevant sources through APIs, web scraping, etc.
$\rightarrow [Collect]$
2.  **Scrub**: Clean data and convert data to machine readable formats. Clearning includes handling missing data, inconsistent labels, or awkward formatting; stripping extraneous characters; normalizing values, etc.
$\rightarrow [Clean, Prepare]$
3.  **Explore**: Find significant patterns and trends using statistical and data analytic methods, such as visualizing, clustering. Also includes transformations of the for more effective analysis, such as dimensionality reduction.
$\rightarrow [Explore]$
4.  **Model**: Construct methods to predict and forecast. These methods include those of inferential statistics and predictive machine learning. 
$\rightarrow [Model]$
5.  **Interpret**: Making sense of the results as well as evaluating the performance of models. May involve domain experts. Also includes methods such as regularization that make models interpretable to those who use them, e.g. scientists or business people.
$\rightarrow [Interpret]$

```{python}

pipes['OSEMI'] = """
Understand | 
Plan | 
Collect | Obtain
Store | 
Clean | Scrub
Explore | Explore
Prepare | Scrub
Model | Model
Interpret | Interpret
Communicate | 
Deploy | 
Reflect | 
"""
```


### Ojeda, et al. on Data Science

Key: `Ojeda+`\
Year: 2014 \
Source: @ojedaPracticalDataScience2014 [URL→](https://www.packtpub.com/product/practical-data-science-cookbook-second-edition/9781787129627)\
Field: Data Science

By 2014, data science had become a widespread practice in industry and the academic, and explanations of its nature became the subject of many books. This text is one of a genre that presents the field as a process, perhaps due to the influence of the CRISP-DM and OSEMI models, and uses the expression pipeline throughout. Note that the model defined in this book is not presented here as canonical. It suffers from various inconsistences, such as the labeling of steps in the text representation of the pipeline versus those on diagrams. It is included to demonstrate the pervasiveness of the model. 

1.  **Acquisition**: Acquire the data from relational databases, NoSQL and document stores, web scraping, distributed databases (e.g. HDFS on a Hadoop platform), RESTful APIs, flat files, etc. Consistent with the other data mining models, the emphasis here is on working with available data, not generating it.
$\rightarrow [Collect]$
2.  **Exploration and understanding**: Understand the data and how it was collected or produced; this often requires significant exploration. Note that this step does *not* correspond to exploration in the sense of exploratory data analysis (EDA). Rather, it reflects the position of the data scientist as the receiver of someone else's data and the need to infer what would normally belong to the first step of the standard squence $Understand$.
$\rightarrow [Understand]$
3.  **Munging, wrangling, and manipulation**: Convert the data into the form required for analysis. This includes a wide range of activities, such as those mentioned in previous models. However, it also conflates the standard phases  $Clean$ and $Prepare$.
$\rightarrow [Clean, Prepare]$
4.  **Analysis and modeling**: Apply statistical and machine learning methods, including clustering, categorization, and classification. One presumes that the standard step of $Explore$ is included here.
$\rightarrow [Explore, Model]$
5.  **Communicating and operationalizing**: At the end of the pipeline, we need to give the data back in a compelling form and structure, sometimes to ourselves to inform the next iteration, and sometimes to a completely different audience. The data products produced can be a simple one-off report or a scalable web product that will be used interactively by millions.
$\rightarrow [Communicate, Deploy]$

```{python}

pipes['Ojeda+'] = """
Understand | Exploration and understanding*
Plan | 
Collect | Acquisition
Store | Acquisition
Clean | Munging, wrangling, and manipulation
Explore | 
Prepare | Munging, wrangling, and manipulatiom
Model | Analysis and modeling
Interpret | 
Communicate | Communicating
Deploy | Operationalizing
Reflect | 
"""
```

### Caffo, et al. on Data Science

Key: `Caffo+` Source: @caffoExecutiveDataScience2015 [URL→](https://leanpub.com/eds)\
Field: Data Science

1.  Question\
2.  Get\
3.  Explore\
4.  Model\
5.  Interpret\
6.  Communicate

```{python}
pipes['Caffo+'] = """
Understand | 
Plan | 
Collect | 
Store | 
Clean | 
Explore | 
Prepare | 
Model | 
Interpret | 
Communicate | 
Deploy | 
Reflect | 
"""
```


### Donaho on Data Science

Key: `Donoho` Source: @donoho50YearsData2017 [URL→](https://doi.org/10.1080/10618600.2017.1384734)

Field: Statistics

1.  Gather
2.  Prepare
3.  Explore
4.  Represent and transform
5.  Compute
6.  Model
7.  Present
8.  Meta

```{python}

pipes['Donoho'] = """
Understand | 
Plan | 
Collect | 
Store | 
Clean | 
Explore | 
Prepare | 
Model | 
Interpret | 
Communicate | 
Deploy | 
Reflect | 
"""
```


### Géron on Machine Learning

Key: `Géron` Source: @geronHandsOnMachineLearning2017 [URL→](https://www.investincotedor.fr/sites/default/files/webform/pdf-hands-on-machine-learning-with-scikit-learn-and-tensorflow-conce-aurlien-gron-pdf-download-free-book-21c7262.pdf)\
Field: Data Science

1.  Big picture\
2.  Get\
3.  Clean\
4.  Discover
5.  Prepare Model
6.  Fine tune\
7.  Launch

```{python}

pipes['Géron'] = """
Understand | 
Plan | 
Collect | 
Store | 
Clean | 
Explore | 
Prepare | 
Model | 
Interpret | 
Communicate | 
Deploy | 
Reflect | 
"""
```


### Das on Data Science

Key: `Das` Source: @dasDataScienceLife2019 [URL→](https://towardsdatascience.com/data-science-life-cycle-101-for-dummies-like-me-e66b47ad8d8f)\
Field: Data Science

1.  Understand
2.  Mine
3.  Clean
4.  Explore
5.  Features
6.  Model
7.  Visualize

```{python}

pipes['Das'] = """
Understand | 
Plan | 
Collect | 
Store | 
Clean | 
Explore | 
Prepare | 
Model | 
Interpret | 
Communicate | 
Deploy | 
Reflect | 
"""
```

### Dataman on Data Science

Key: `Dataman` Source: @datamanDataScienceModeling2020 [URL→](https://towardsdatascience.com/data-science-modeling-process-fa6e8e45bf02)\
Field: Data Science

1.  Business
2.  Data requirements\
3.  Collection
4.  EDA
5.  Modeling
6.  Evaluation
7.  Deployment
8.  Monitoring

```{python}

pipes['Dataman'] = """
Understand | 
Plan | 
Collect | 
Store | 
Clean | 
Explore | 
Prepare | 
Model | 
Interpret | 
Communicate | 
Deploy | 
Reflect | 
"""
```


### Porter on Data Science

Key: `Porter` Source: @porterFrameworkDataScience2020 Field: Statistics <!--
file:///private/var/folders/14/rnyfspnx2q131jp_752t9fc80000gn/T/com.microsoft.Outlook/Outlook%20Temp/data-science%5B44%5D.html#categories_of_data_science
-->

1.  Collect\
2.  Store and represent\
3.  Manipulation\
4.  Computing\
5.  Analytics\
6.  Communicate\
7.  Practice
8.  Disciplinary

```{python}

pipes['Porter'] = """
Understand | 
Plan | 
Collect | 
Store | 
Clean | 
Explore | 
Prepare | 
Model | 
Interpret | 
Communicate | 
Deploy | 
Reflect | 
"""
```


## Comments

* Place of clustering
* Place of explorations
* Cycles and pipes
* Missing steps, implicit steps
* Differences in order mostly confined to groups
* Right even when wrong -- the end points $I$ and $V$, and the middle $III$,  remain constant; the arms, as it were, are where it gets messy.

## Summary Table

```{python}
 
import pandas as pd
import re
```

```{python}

data = []
cols = ['Source', 'Std Phase', 'Src Phase']
for source in pipes:
    for row in pipes[source].lower().split("\n")[1:-1]:
        data.append([source] + re.split('\s*\|\s*', row))

# Convert CSV to wide table
df = pd.DataFrame(data, columns=cols).reset_index(drop=True).set_index(cols[:2]).unstack() 
df.columns = df.columns.droplevel(0)

# Preserve sorting
df = df[[phase.lower() for phase in phases]]
df = df.T[sources].T

df.columns.name = ''
df.index.name = ''
```

```{python}
#| output: true 
df.style.bar()
```


## Extra

```{python}

my_table = """
|         | Understand  | Plan              | Collect    | Store               | Clean         | Explore                | Prepare                          | Model             | Interpret    | Communicate | Deploy                 | Reflect      |
|---------|-------------|-------------------|------------|---------------------|---------------|------------------------|----------------------------------|-------------------|--------------|-------------|------------------------|--------------|
| Tukey   |             | Plan              | Gather     |                     |               | Analyze                |                                  |                   | Interpret    |             |                        |              |
| KDD     |             |                   | Selection  |                     | Preprocessing |                        | Transformation                   | Data mining       | Interpreting |             |                        |              |
| SEMMA   |             |                   | Sample     |                     |               | Explore                | Modify                           | Model             | Assess       |             |                        |              |
| Hayashi | Design      |                   | Collect    |                     |               |                        |                                  | Analyze           |              |             |                        |              |
| CRISPDM | Business    |                   | Collect    |                     |               | Explore                | Preparation                      | Modeling          | Evaluation   |             | Deployment             |              |
| OSEMI   |             |                   | Obtain     |                     | Scrub         | Explore                |                                  | Model             | Interpret    |             |                        |              |
| Ojeda+  |             |                   | Acquire    |                     |               | Explore and understand | Wrangle                          | Analyze and model |              | Communicate | Operationalize         |              |
| Caffo+  | Question    |                   | Get        |                     |               | Explore                |                                  | Model             | Interpret    | Communicate |                        |              |
| Donaho  |             |                   | Gather     |                     | Prepare       | Explore                | Represent and transform; Compute | Model             |              | Present     |                        | Meta         |
| Géron   | Big picture |                   | Get        |                     | Clean         | Discover               | Prepare                          | Model; Fine tune  |              |             | Launch                 |              |
| Das     | Understand  |                   | Mine       |                     | Clean         | Explore                | Features                         | Model             |              | Visualize   |                        |              |
| Dataman | Business    | Data requirements | Collection |                     |               | EDA                    |                                  | Modeling          | Evaluation   |             | Deployment; Monitoring |              |
| Porter  |             |                   | Collect    | Store and represent | Manipulation  |                        | Computing                        | Analytics         |              | Communicate | Practice               | Disciplinary |
"""
```

```{python}

# for i, row in enumerate(my_table.split("\n")[1:-1]):
#     clean_row = [item.strip() for item in row.split("|")[1:-1]]
#     print('\n'.join(clean_row))
#     print('-' * 40)
```

## References

::: {#refs}
:::

## Test HTML

```{python}
#| output: true 
#| fig-cap: "Composite Timeline"
# print(df.to_html())
```
